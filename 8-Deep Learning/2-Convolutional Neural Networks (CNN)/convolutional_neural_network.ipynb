{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DR-eO17geWu"
   },
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EMefrVPCg-60"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sahil\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.16.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxQxCBWyoGPE"
   },
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvE-heJNo3GG"
   },
   "source": [
    "### Preprocessing the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 files belonging to 2 classes.\n",
      "WARNING:tensorflow:From C:\\Users\\sahil\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data = image_dataset_from_directory('dataset/training_set',\n",
    "                                             image_size=(64, 64),\n",
    "                                             batch_size=32,\n",
    "                                             label_mode='binary')\n",
    "\n",
    "train_datagen = tf.keras.Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.Rescaling(1./255),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomZoom(0.2),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomTranslation(0.2, 0.2)\n",
    "])\n",
    "\n",
    "train_set = training_data.map(\n",
    "    lambda x, y: (train_datagen(x, training=True), y)\n",
    ")\n",
    "#train_set = train_set.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrCMmGw9pHys"
   },
   "source": [
    "### Preprocessing the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data = image_dataset_from_directory('dataset/test_set',\n",
    "                                             image_size=(64, 64),  # Size to which all images will be resized\n",
    "                                             batch_size=32,\n",
    "                                             label_mode='binary')\n",
    "\n",
    "test_datagen = tf.keras.Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "])\n",
    "\n",
    "test_set = test_data.map(\n",
    "    lambda x,y: (test_datagen(x, training= False), y)\n",
    ")\n",
    "\n",
    "#test_set = test_set.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "af8O4l90gk7B"
   },
   "source": [
    "## Part 2 - Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ces1gXY2lmoX"
   },
   "source": [
    "### Initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5YJj_XMl5LF"
   },
   "source": [
    "### Step 1 - Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size = 3, activation='relu', input_shape = [64,64,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tf87FpvxmNOJ"
   },
   "source": [
    "### Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sahil\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xaTOgD8rm4mU"
   },
   "source": [
    "### Adding a second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size = 3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tmiEuvTunKfk"
   },
   "source": [
    "### Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dAoSECOm203v"
   },
   "source": [
    "### Step 4 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units = 128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yTldFvbX28Na"
   },
   "source": [
    "### Step 5 - Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units = 1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D6XkI90snSDl"
   },
   "source": [
    "## Part 3 - Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfrFQACEnc6i"
   },
   "source": [
    "### Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sahil\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\optimizers\\__init__.py:317: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehS-v3MIpX2h"
   },
   "source": [
    "### Training the CNN on the Training set and evaluating it on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "WARNING:tensorflow:From C:\\Users\\sahil\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sahil\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "250/250 [==============================] - 35s 79ms/step - loss: 0.6879 - accuracy: 0.5493 - val_loss: 0.6676 - val_accuracy: 0.6045\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 0.6463 - accuracy: 0.6181 - val_loss: 0.5944 - val_accuracy: 0.6850\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 0.6131 - accuracy: 0.6600 - val_loss: 0.5972 - val_accuracy: 0.6845\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 0.5998 - accuracy: 0.6745 - val_loss: 0.6020 - val_accuracy: 0.6695\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 0.5857 - accuracy: 0.6851 - val_loss: 0.5360 - val_accuracy: 0.7270\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 0.5698 - accuracy: 0.7003 - val_loss: 0.5370 - val_accuracy: 0.7280\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 0.5569 - accuracy: 0.7151 - val_loss: 0.5109 - val_accuracy: 0.7425\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 0.5469 - accuracy: 0.7232 - val_loss: 0.5133 - val_accuracy: 0.7440\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 0.5447 - accuracy: 0.7228 - val_loss: 0.4880 - val_accuracy: 0.7675\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 0.5316 - accuracy: 0.7361 - val_loss: 0.5090 - val_accuracy: 0.7480\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 0.5285 - accuracy: 0.7390 - val_loss: 0.4854 - val_accuracy: 0.7635\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 0.5163 - accuracy: 0.7410 - val_loss: 0.5201 - val_accuracy: 0.7450\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 0.5138 - accuracy: 0.7449 - val_loss: 0.4550 - val_accuracy: 0.7915\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 0.5054 - accuracy: 0.7462 - val_loss: 0.6121 - val_accuracy: 0.6935\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 0.4982 - accuracy: 0.7575 - val_loss: 0.4522 - val_accuracy: 0.7940\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 0.5026 - accuracy: 0.7565 - val_loss: 0.4492 - val_accuracy: 0.8015\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 0.4896 - accuracy: 0.7625 - val_loss: 0.4480 - val_accuracy: 0.7875\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 0.4805 - accuracy: 0.7673 - val_loss: 0.4783 - val_accuracy: 0.7775\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 0.4848 - accuracy: 0.7675 - val_loss: 0.4476 - val_accuracy: 0.7840\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 0.4812 - accuracy: 0.7676 - val_loss: 0.4399 - val_accuracy: 0.8010\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 0.4689 - accuracy: 0.7731 - val_loss: 0.4388 - val_accuracy: 0.7920\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 0.4710 - accuracy: 0.7720 - val_loss: 0.4295 - val_accuracy: 0.8055\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 0.4573 - accuracy: 0.7807 - val_loss: 0.4557 - val_accuracy: 0.7980\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 0.4550 - accuracy: 0.7890 - val_loss: 0.4378 - val_accuracy: 0.8075\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 0.4524 - accuracy: 0.7864 - val_loss: 0.4289 - val_accuracy: 0.8060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x144e76dfb10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = train_set, validation_data = test_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3PZasO0006Z"
   },
   "source": [
    "## Part 4 - Making a single prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cats', 'dogs']\n"
     ]
    }
   ],
   "source": [
    "classes_indices = training_data.class_names\n",
    "print(classes_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "[[1.]]\n",
      "dog\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "[[1.]]\n",
      "dog\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "[[0.]]\n",
      "cat\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "[[1.]]\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,5):\n",
    "    test_image = image.load_img(f\"dataset/single_prediction/cat_or_dog_{i}.jpg\", target_size = (64, 64))\n",
    "    test_image = image.img_to_array(test_image)\n",
    "    test_image = np.expand_dims(test_image, axis = 0)\n",
    "    result = cnn.predict(test_image)\n",
    "    print(result)\n",
    "    if result[0][0] == 1:\n",
    "        prediction = 'dog'\n",
    "    else:\n",
    "        prediction = 'cat'\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahil\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\engine\\training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "cnn.save('cat_or_dog_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save the model in the TensorFlow SavedModel format\n",
    "cnn.save('my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Assuming your trained model is stored in the variable `cnn`\n",
    "model = cnn\n",
    "\n",
    "# Define class indices (this should match how you defined them during training)\n",
    "class_indices = {'cat': 0, 'dog': 1}\n",
    "index_to_class = {v: k for k, v in class_indices.items()}\n",
    "\n",
    "def preprocess_frame(frame, img_size=(64, 64)):\n",
    "    img_array = cv2.resize(frame, img_size)\n",
    "    img_array = img_array.astype('float32') / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    return img_array\n",
    "\n",
    "def predict_frame(model, frame):\n",
    "    preprocessed_frame = preprocess_frame(frame)\n",
    "    prediction = model.predict(preprocessed_frame)\n",
    "    predicted_class_index = int(np.round(prediction[0][0]))  # Assuming binary classification with sigmoid activation\n",
    "    predicted_class_label = index_to_class[predicted_class_index]\n",
    "    return predicted_class_label\n",
    "\n",
    "# Start video capture from camera\n",
    "cap = cv2.VideoCapture(0)  # Use 0 for the default camera\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Make prediction\n",
    "    predicted_class_label = predict_frame(model, frame)\n",
    "    \n",
    "    # Display the result\n",
    "    cv2.putText(frame, f'Prediction: {predicted_class_label}', (10, 30), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    cv2.imshow('Live Dog/Cat Classifier', frame)\n",
    "    \n",
    "    # Break loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sahil\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\saving\\legacy\\saved_model\\load.py:109: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sahil\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\saving\\legacy\\saved_model\\load.py:109: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unsupported image shape: (1, 64, 64, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m cropped_frame \u001b[38;5;241m=\u001b[39m frame[startY:endY, startX:endX]\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Classify the cropped region using the CNN model\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m predicted_class_label \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcropped_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Draw bounding box and label on the frame\u001b[39;00m\n\u001b[0;32m     62\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_class_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfidence\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[20], line 19\u001b[0m, in \u001b[0;36mpredict_frame\u001b[1;34m(model, frame)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_frame\u001b[39m(model, frame):\n\u001b[1;32m---> 19\u001b[0m     preprocessed_frame \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(preprocessed_frame)\n\u001b[0;32m     21\u001b[0m     predicted_class_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mround(prediction[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]))  \u001b[38;5;66;03m# Assuming binary classification with sigmoid activation\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[20], line 14\u001b[0m, in \u001b[0;36mpreprocess_frame\u001b[1;34m(frame, img_size)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_frame\u001b[39m(frame, img_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m)):\n\u001b[0;32m     13\u001b[0m     img_array \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(frame, img_size)\n\u001b[1;32m---> 14\u001b[0m     img_array \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_to_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     img_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(test_image, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img_array\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\image_utils.py:158\u001b[0m, in \u001b[0;36mimg_to_array\u001b[1;34m(img, data_format, dtype)\u001b[0m\n\u001b[0;32m    156\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape((x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported image shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[1;31mValueError\u001b[0m: Unsupported image shape: (1, 64, 64, 3)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load your trained model (assuming it's already saved)\n",
    "cnn = tf.keras.models.load_model('my_model')\n",
    "\n",
    "# Define class indices (this should match how you defined them during training)\n",
    "class_indices = {'cat': 0, 'dog': 1}\n",
    "index_to_class = {v: k for k, v in class_indices.items()}\n",
    "\n",
    "def preprocess_frame(frame, img_size=(64, 64)):\n",
    "    img_array = cv2.resize(frame, img_size)\n",
    "    img_array = image.img_to_array(test_image)\n",
    "    img_array = np.expand_dims(test_image, axis = 0)\n",
    "    return img_array\n",
    "\n",
    "def predict_frame(model, frame):\n",
    "    preprocessed_frame = preprocess_frame(frame)\n",
    "    prediction = model.predict(preprocessed_frame)\n",
    "    predicted_class_index = int(np.round(prediction[0][0]))  # Assuming binary classification with sigmoid activation\n",
    "    predicted_class_label = index_to_class[predicted_class_index]\n",
    "    return predicted_class_label\n",
    "\n",
    "# Load the pre-trained MobileNet SSD model for object detection\n",
    "prototxt_path = 'deploy.prototxt'\n",
    "caffemodel_path = 'mobilenet_iter_73000.caffemodel'\n",
    "net = cv2.dnn.readNetFromCaffe(prototxt_path, caffemodel_path)\n",
    "\n",
    "# Start video capture from camera\n",
    "cap = cv2.VideoCapture(0)  # Use 0 for the default camera\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Prepare the frame for object detection\n",
    "    (h, w) = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 0.007843, (300, 300), 127.5)\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > 0.2:  # Confidence threshold\n",
    "            # Extract the index of the class label from the detections\n",
    "            idx = int(detections[0, 0, i, 1])\n",
    "\n",
    "            # If the detected object is a dog or cat (adjust these class indices)\n",
    "            if idx in [15, 3]:  # Adjust this based on the object detector's class labels for 'dog' and 'cat'\n",
    "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "                # Crop the detected region\n",
    "                cropped_frame = frame[startY:endY, startX:endX]\n",
    "\n",
    "                # Classify the cropped region using the CNN model\n",
    "                predicted_class_label = predict_frame(cnn, cropped_frame)\n",
    "\n",
    "                # Draw bounding box and label on the frame\n",
    "                label = f\"{predicted_class_label}: {confidence:.2f}\"\n",
    "                cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "                y = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "                cv2.putText(frame, label, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the output frame\n",
    "    cv2.imshow('Live Dog/Cat Classifier', frame)\n",
    "\n",
    "    # Break loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "convolutional_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
